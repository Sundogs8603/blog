# 值得关注的对中文支持较好的开源模型

> Author: **ninehills**  
> Labels: **blog**  
> Created: **2023-06-21T06:15:49Z**  
> Updated: **2023-07-13T04:24:34Z**  
> Link and comments: <https://github.com/ninehills/blog/issues/94>  


> Updated  
> - 2023.7.13： 增加 baichuan-13B-Chat、InternLM 模型
> - 2023.6.25: 增加 ChatGLM2-6B、Vicuna-33B-v1.3  模型
> - 2023.6.24: 增加 MPT-30B/MPT-30B-Chat 模型

## 模型推理

建议使用通用的模型推理工具包运行推理，一般都提供较好的UI以及兼容OpenAI 的API。常见的有：

- https://github.com/lm-sys/FastChat
- https://github.com/oobabooga/text-generation-webui
- https://github.com/ggerganov/llama.cpp
- https://github.com/nomic-ai/gpt4all
- https://github.com/vllm-project/vllm

其中

1. 兼容性最好的是 text-generation-webui，支持 8bit/4bit 量化加载、GPTQ 模型加载、GGML 模型加载、Lora 权重合并、OpenAI 兼容API、Embeddings模型加载等功能，推荐！
2. 苹果 M 系列芯片，推荐用 llama.cpp
3. 开箱即用，选择 gpt4all，有桌面端软件。

> 注：如果模型参数过大无法加载，可以在 HuggingFace 上寻找其 GPTQ 4-bit 版本，或者 GGML 版本（支持Apple M系列芯片）。
> 目前30B规模参数模型的 GPTQ 4-bit 量化版本，可以在 24G显存的 3090/4090 显卡上单卡运行推理。

## 预训练模型

值得关注的支持中文的开源 Pretrain 模型（同类模型中选效果最好的、参数最大的）

| 名称                       | 类型     | 参数量 | 上下文      | License | 地址                                                                              |
| -------------------------- | -------- | ------ | ----------- | ------- | --------------------------------------------------------------------------------- |
| Aquila-7B                  | Pretrain | 7B     | 2048 tokens | 商用    | https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila/Aquila-pretrain |
| Ziya-LLaMA-13B-Pretrain-v1 | Pretrain | 13B    | 2048 tokens | 非商用  | https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-Pretrain-v1                       |
| Chinese-LLaMA-33B          | Pretrain | 33B    | 2048 tokens | 非商用  | https://github.com/ymcui/Chinese-LLaMA-Alpaca                                     |
|  Baichuan-13B     |    Pretrain  |  13B   |    4096 tokens     |   商用（需申请）    |        https://github.com/baichuan-inc/baichuan-13B     |
| MPT-30B | Pretrain | 30B | 8192 tokens | 商用 | https://huggingface.co/mosaicml/mpt-30b |
| intern-7b | Pretrain | 7B | 2048 tokens | 商用（需申请）| https://github.com/InternLM/InternLM |

> 不同模型的 token 对应的中文字符数不同，一般来说专门为中文设计的模型，其词表较大，相同中文字符数使用的 token 数量相比于 OpenAI （1 字符 2 token）较小，约为平均 1 字符 1 token。

## Chat 模型

值得关注的支持中文的开源 Chat 模型（同类模型中选效果最好的）

| 名称                    | 类型 | 参数量 | 上下文      | License | 地址                                                                          |
| ----------------------- | ---- | ------ | ----------- | ------- | ----------------------------------------------------------------------------- |
| BELLE-LLaMA-EXT-13B     | SFT  | 13B    | 2048 tokens | 非商用  | https://github.com/LianjiaTech/BELLE                                          |
| Baichuan-13B-Chat | SFT | 13B | 4096 tokens | 商用（需申请）| https://huggingface.co/baichuan-inc/Baichuan-13B-Chat |
| Chinese-Alpaca-Plus-13B | SFT  | 13B    | 2048 tokens | 非商用  | https://github.com/ymcui/Chinese-LLaMA-Alpaca                                 |
| ChatGLM-6B              | SFT  | 6B     | 2048 tokens | 非商用  | https://github.com/THUDM/ChatGLM-6B                                           |
| ChatGLM2-6B | SFT | 6B | 32K/8K** | 商用（需申请） | https://github.com/THUDM/ChatGLM2-6B |
| Ziya-LLaMA-13B-v1.1     | SFT  | 13B    | 2048 tokens | 非商用  | https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1.1                          |
| AquilaChat-7B           | SFT  | 7B     | 2048 tokens | 商用    | https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila/Aquila-chat |
|  WizardLM-30B-V1.0   |  SFT |  30B |  2048 tokens |  非商用  | https://huggingface.co/WizardLM/WizardLM-30B-V1.0     |
| MPT-30B-Chat | SFT | 30B | 8192 tokens | 非商用* | https://huggingface.co/mosaicml/mpt-30b-chat |
| Vicuna-33B-v1.3 | SFT | 33B | 2048 tokens | 非商用 | https://huggingface.co/lmsys/vicuna-33b-v1.3 |
| intern-chat-7b-8k | SFT | 7B | 8192 tokens | 商用（需申请）| https://github.com/InternLM/InternLM |

> `*` MPT-30B-Chat 因为在 SFT 过程中使用了非商用的数据集，所以其不可商用。但是其基础模型可以商用，且微调训练代码和过程公开，可自行 SFT 出可商用的 Chat 模型。
> `**` 当前版本的 ChatGLM2-6B 对单轮超长文档的理解能力有限。

## Embeddings 模型

用来替代 OpenAI 的 Embeddings 模型：

- [m3e-base](https://huggingface.co/moka-ai/m3e-base): 效果最好，非商用
- [text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)：可商用

<img width="736" alt="image" src="https://github.com/ninehills/ninehills.github.io/assets/270298/b9fcaefd-bac0-4094-91c0-b616a08a8696">
